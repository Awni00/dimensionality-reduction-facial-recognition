{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Generalized Fisherface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Fisherface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: OPTIMIZATION SEEMS TO BE SOMEHOW NOT WELL POSED. WHY? NO BOUNDS, LOSS IS ARBITRARILY SMALL / MAXIMIZATION ARBITRARILY LARGE\n",
    "\n",
    "Q: Is fisherace omitting some details about the optimization? what's to stop us from getting arbitrarily low loss through an arbitrarily large matrix? is the optimization supposed to be over matrices with columns with unit length? If so, how can I turn this into an unconstrained optimization problem so that I can use gradient descent or similar optimization algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA (via Gradient Decent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myPCA():\n",
    "    '''An implementation of PCA using gradient decent'''\n",
    "    # used for debugging implementation of generalized fisherface...\n",
    "    def __init__(self, X, n_components, optimizer=torch.optim.Adam, lr=1e-2, n_steps=100, l2_coef=1, verbose=True, **optim_kwargs):\n",
    "\n",
    "        self.n_samples = np.shape(X)[0]\n",
    "        self.dim = np.shape(X)[1]\n",
    "        self.n_components = n_components\n",
    "\n",
    "        self.l2_coef = l2_coef\n",
    "\n",
    "        W = torch.eye(self.dim, self.n_components) # (dim x n_compoenents) matrix\n",
    "        self.W = nn.Parameter(W)\n",
    "\n",
    "        # NOTE: we assume normalization prior to calling this...\n",
    "        global_mean = np.mean(X, axis=0)\n",
    "\n",
    "        # compute total scatter matrix\n",
    "        self.S_T = (X - global_mean).T @ (X - global_mean)\n",
    "        # self.S_T = np.cov(X.T)# NOTE TEMP\n",
    "        self.S_T = torch.tensor(self.S_T, dtype=torch.float32)\n",
    "\n",
    "        self.optimizer = optimizer([self.W], lr=lr, **optim_kwargs)\n",
    "        self.n_steps = n_steps\n",
    "        self.loss_history = []\n",
    "\n",
    "    def loss(self):\n",
    "        # (negative since we're maximizing) + L2 regularization to implement constrained optimization (NOTE TEMP experimentating)\n",
    "        loss = - torch.det(self.W.t() @ self.S_T @ self.W) + self.l2_coef * torch.linalg.matrix_norm(self.W, ord=2)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def W_opt(self):\n",
    "        '''returns normalized transformation matrix'''\n",
    "        return self.W / torch.sqrt(torch.sum(self.W**2, axis=0))\n",
    "\n",
    "    def fit(self, X=None, y=None, n_steps=None):\n",
    "        if n_steps is None:\n",
    "            n_steps = self.n_steps\n",
    "\n",
    "        print('Fitting transformation...')\n",
    "        for _ in tqdm(range(n_steps)):\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.loss()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.loss_history.append(loss.detach().numpy())\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.W_opt.t() @ X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "dim = 64*64\n",
    "n_classes = 6\n",
    "n_components = 1\n",
    "\n",
    "X1 = np.random.uniform(size=n_samples)\n",
    "X2 = 2*X1 + 0.2*np.random.normal(size=n_samples)\n",
    "X = np.array((X1, X2)).T\n",
    "\n",
    "# X = np.random.uniform(size=(n_samples, dim))\n",
    "y = np.random.randint(0, n_classes-1, size=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = myPCA(X, n_components, optimizer=torch.optim.RMSprop, lr=1e-2, l2_coef=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting transformation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:01<00:00, 848.35it/s]\n"
     ]
    }
   ],
   "source": [
    "pca.fit(n_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7320],\n",
       "        [0.6813]], grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.W_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e820b76fa0>]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pca.loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.W_opt.t() @ pca.W_opt # W_opt is indeed orthonormal..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = PCA(n_components=n_components).fit(X).components_.T # get the transformation obtained by sklearn's PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.41433838]\n",
      " [-0.91012291]]\n"
     ]
    }
   ],
   "source": [
    "print(W) # but is different from the transformation obtained via PCA; but it might be 'equivalent'? perhaps same 'subspace'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the results from sklearn's PCA are different. but sklearn's PCA is different each time you run it. how can we validate that the results are 'equivalent'?? compare subspaces?? compare loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(444.9521)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = torch.tensor(W, dtype=torch.float32)\n",
    "torch.det(W.T @ pca.S_T @ W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-146396.6719, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.loss() # loss with un-normalized transformation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-379.3869, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "- torch.det(pca.W_opt.t() @ pca.S_T @ pca.W_opt) + torch.linalg.matrix_norm(pca.W_opt, ord=2) # full loss (with regualarization) of normalized matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the loss obtained by sklearn's PCA implementation is consistently the same even though the `components_` differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(289.9221, grad_fn=<DetLuBasedHelperBackward0>)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.det(pca.W_opt.t() @ pca.S_T @ pca.W_opt) # only maximization functional from fisherface paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my implementation doesn't achieve as high a maximization as sklearn's PCA... why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "actually, for a simple example, my implementation achieaves the exact same loss... is there some kind of instability going on? convergence is achieved for simple examples but for more complicated ones? confirm: this is a convex optimization problem right? should i be looking into optimization algorithms other than gradient decent-based ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fisherface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fisherface():\n",
    "    def __init__(self, X, y, n_components=None, pca_first=True, verbose=True, S_B=None, S_W=None):\n",
    "\n",
    "        self.n_samples = np.shape(X)[0]\n",
    "        self.dim = np.shape(X)[1]\n",
    "\n",
    "        self.n_classes = len(np.unique(y))\n",
    "\n",
    "\n",
    "        # set default number components to be number of classes - 1 (if no other number given)\n",
    "        if n_components is None:\n",
    "            # this is the maximum number of components s.t. S_W is non-singular\n",
    "            self.n_components = self.n_classes - 1\n",
    "        else:\n",
    "            self.n_components = n_components\n",
    "\n",
    "        if pca_first:\n",
    "            # perform PCA first to address S_W, the within-class scatter matrix, being singular\n",
    "\n",
    "            # the PCA matrix\n",
    "            if verbose:\n",
    "                print('Performing PCA for initial dimensionality reduction')\n",
    "            pca = PCA(n_components=self.n_samples - self.n_classes).fit(X)\n",
    "            self.W_pca = torch.tensor(pca.components_.T, dtype=torch.float32) # (dim x n_samples - n_classes) matrix\n",
    "            if verbose:\n",
    "                print('Done.')\n",
    "\n",
    "            # the fisher's linear discriminant matrix\n",
    "            # TODO: consider what the best choice for intialization would be.\n",
    "            W_fld = torch.eye(self.n_samples - self.n_classes, self.n_components) \\\n",
    "                    + torch.rand(self.n_samples - self.n_classes, self.n_components) # (n_samples - n_classes x n_compoenents) matrix\n",
    "            self.W_fld = nn.Parameter(W_fld)\n",
    "\n",
    "        else:\n",
    "            # don't perform PCA, perform standard fisher's linear discriminant\n",
    "            self.W_pca = torch.eye(self.dim)\n",
    "\n",
    "            W_fld = torch.Tensor(self.n_samples - self.n_classes, self.n_components) # (n_samples - n_classes x n_compoenents) matrix\n",
    "            self.W_fld = nn.Parameter(W_fld)\n",
    "\n",
    "\n",
    "        # NOTE: we assume normalization prior to calling this...\n",
    "        classes = np.sort(np.unique(y)) # classes in the dataset\n",
    "        points_by_class = [X[y==class_] for class_ in classes] # the sets X_i\n",
    "        n_points_by_class = [len(X[y==class_]) for class_ in classes]\n",
    "        class_means = [np.mean(class_points, axis=0) for class_points in points_by_class] # mu_i's\n",
    "        global_mean = np.expand_dims(np.mean(X, axis=0),-1)\n",
    "\n",
    "        # between-class scatter matrix\n",
    "        if verbose:\n",
    "            print('Computing the between-class and within-class scatter matrices...')\n",
    "\n",
    "        # allow for precomputing S_B and S_W for faster debugging\n",
    "        if S_B is None or S_W is None:\n",
    "            self.S_B = np.sum([n_points_by_class[class_] * (class_means[class_] - global_mean).T @ (class_means[class_] - global_mean) for class_ in classes], axis=0)\n",
    "            self.S_W = np.sum([(points_by_class[class_] - class_means[class_]).T @ (points_by_class[class_] - class_means[class_]) for class_ in classes], axis=0)\n",
    "        else:\n",
    "            self.S_B = S_B\n",
    "            self.S_W = S_W\n",
    "\n",
    "        self.S_B = torch.tensor(self.S_B, dtype=torch.float32)\n",
    "        self.S_W = torch.tensor(self.S_W, dtype=torch.float32)\n",
    "        if verbose:\n",
    "            print('Done.')\n",
    "\n",
    "\n",
    "    def loss(self):\n",
    "        numerator = torch.det(self.W_fld.t() @ self.W_pca.t() @ self.S_B @ self.W_pca @ self.W_fld)\n",
    "        denominator = torch.det(self.W_fld.t() @ self.W_pca.t() @ self.S_W @ self.W_pca @ self.W_fld)\n",
    "        loss = - numerator / denominator # (negative since we're maximizing)\n",
    "        return loss\n",
    "\n",
    "    @property\n",
    "    def W_opt(self):\n",
    "        return self.W_pca @ self.W_fld\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        pass\n",
    "\n",
    "    def transform(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_B = fld.S_B.detach().numpy()\n",
    "S_W = fld.S_W.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "dim = 64*64\n",
    "n_classes = 6\n",
    "n_components = 5\n",
    "\n",
    "X = np.random.uniform(size=(n_samples, dim))\n",
    "y = np.random.randint(0, n_classes-1, size=n_samples)\n",
    "\n",
    "# fld = Fisherface(X, y, n_components=n_components, pca_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing PCA for initial dimensionality reduction\n",
      "Done.\n",
      "Computing the between-class and within-class scatter matrices...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "fld = Fisherface(X, y, n_components=n_components, pca_first=True, S_B=S_B, S_W=S_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 61853.2500, -52495.2578, -39789.2031,  13971.1719, -21186.1426],\n",
       "        [ -5974.6821,   8014.5391,  10462.8730,   2579.0161,  -3760.1821],\n",
       "        [ 10566.6855, -35335.0898, -19384.4062,   2742.2375,   4167.3550],\n",
       "        ...,\n",
       "        [-11012.6816,   7640.9126,  13530.7695,  -8384.8271,   6211.9663],\n",
       "        [ 14166.7910, -30040.1055, -10209.0518,   2564.4790,  -1195.6466],\n",
       "        [ 24937.6035, -18554.4766, -13991.1953,  -3230.4907,  -7145.3887]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = fld.loss()\n",
    "loss.backward()\n",
    "fld.W_fld.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[1.2030, 0.1466, 0.0552, 0.3083, 0.6972],\n",
       "        [0.6202, 1.8754, 0.2999, 0.8033, 0.2063],\n",
       "        [0.7881, 0.0266, 1.0428, 0.4321, 0.4302],\n",
       "        ...,\n",
       "        [0.6960, 0.2532, 0.1509, 0.1046, 0.1454],\n",
       "        [0.8285, 0.1132, 0.5184, 0.6016, 0.3878],\n",
       "        [0.7546, 0.6600, 0.0294, 0.3034, 0.6303]], requires_grad=True)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fld.W_fld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:09<00:00, 10.78it/s]\n"
     ]
    }
   ],
   "source": [
    "n_steps = int(1e2)\n",
    "optimizer = torch.optim.SGD([fld.W_fld], lr=1e-10)\n",
    "loss_history = []\n",
    "\n",
    "\n",
    "for _ in tqdm(range(n_steps)):\n",
    "    optimizer.zero_grad()\n",
    "    loss = fld.loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    loss_history.append(loss.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(-98863.695, dtype=float32),\n",
       " array(-99451.12, dtype=float32),\n",
       " array(-100042.71, dtype=float32),\n",
       " array(-100643.08, dtype=float32),\n",
       " array(-101239.836, dtype=float32),\n",
       " array(-101846.47, dtype=float32),\n",
       " array(-102453.01, dtype=float32),\n",
       " array(-103061.99, dtype=float32),\n",
       " array(-103679., dtype=float32),\n",
       " array(-104297.48, dtype=float32),\n",
       " array(-104920.69, dtype=float32),\n",
       " array(-105559.375, dtype=float32),\n",
       " array(-106183.84, dtype=float32),\n",
       " array(-106816.484, dtype=float32),\n",
       " array(-107460.91, dtype=float32),\n",
       " array(-108106.38, dtype=float32),\n",
       " array(-108752.016, dtype=float32),\n",
       " array(-109408.51, dtype=float32),\n",
       " array(-110066.05, dtype=float32),\n",
       " array(-110721.4, dtype=float32),\n",
       " array(-111395.96, dtype=float32),\n",
       " array(-112067.125, dtype=float32),\n",
       " array(-112738.445, dtype=float32),\n",
       " array(-113422.195, dtype=float32),\n",
       " array(-114103.32, dtype=float32),\n",
       " array(-114793.85, dtype=float32),\n",
       " array(-115487.945, dtype=float32),\n",
       " array(-116189.01, dtype=float32),\n",
       " array(-116891.58, dtype=float32),\n",
       " array(-117595.91, dtype=float32),\n",
       " array(-118308.016, dtype=float32),\n",
       " array(-119024.25, dtype=float32),\n",
       " array(-119748.62, dtype=float32),\n",
       " array(-120478.336, dtype=float32),\n",
       " array(-121201.305, dtype=float32),\n",
       " array(-121949.01, dtype=float32),\n",
       " array(-122689.7, dtype=float32),\n",
       " array(-123438.52, dtype=float32),\n",
       " array(-124185.02, dtype=float32),\n",
       " array(-124940.51, dtype=float32),\n",
       " array(-125706.16, dtype=float32),\n",
       " array(-126469.3, dtype=float32),\n",
       " array(-127244.63, dtype=float32),\n",
       " array(-128027.59, dtype=float32),\n",
       " array(-128807.516, dtype=float32),\n",
       " array(-129603.26, dtype=float32),\n",
       " array(-130395.484, dtype=float32),\n",
       " array(-131195.67, dtype=float32),\n",
       " array(-131994.45, dtype=float32),\n",
       " array(-132804.16, dtype=float32),\n",
       " array(-133621.31, dtype=float32),\n",
       " array(-134452.3, dtype=float32),\n",
       " array(-135274.83, dtype=float32),\n",
       " array(-136110.67, dtype=float32),\n",
       " array(-136956.39, dtype=float32),\n",
       " array(-137805.05, dtype=float32),\n",
       " array(-138641.23, dtype=float32),\n",
       " array(-139502.14, dtype=float32),\n",
       " array(-140369.73, dtype=float32),\n",
       " array(-141235.05, dtype=float32),\n",
       " array(-142112.84, dtype=float32),\n",
       " array(-142998.33, dtype=float32),\n",
       " array(-143880.03, dtype=float32),\n",
       " array(-144781.81, dtype=float32),\n",
       " array(-145684.06, dtype=float32),\n",
       " array(-146577.7, dtype=float32),\n",
       " array(-147494.39, dtype=float32),\n",
       " array(-148417.4, dtype=float32),\n",
       " array(-149348.84, dtype=float32),\n",
       " array(-150269.7, dtype=float32),\n",
       " array(-151210.14, dtype=float32),\n",
       " array(-152163.2, dtype=float32),\n",
       " array(-153113.25, dtype=float32),\n",
       " array(-154066.86, dtype=float32),\n",
       " array(-155035.39, dtype=float32),\n",
       " array(-156008.98, dtype=float32),\n",
       " array(-156986.78, dtype=float32),\n",
       " array(-157968.6, dtype=float32),\n",
       " array(-158978.61, dtype=float32),\n",
       " array(-159976.42, dtype=float32),\n",
       " array(-160986.44, dtype=float32),\n",
       " array(-162002.73, dtype=float32),\n",
       " array(-163015., dtype=float32),\n",
       " array(-164051.77, dtype=float32),\n",
       " array(-165092.61, dtype=float32),\n",
       " array(-166136.36, dtype=float32),\n",
       " array(-167177.52, dtype=float32),\n",
       " array(-168251.8, dtype=float32),\n",
       " array(-169325.34, dtype=float32),\n",
       " array(-170405.69, dtype=float32),\n",
       " array(-171479.14, dtype=float32),\n",
       " array(-172575.77, dtype=float32),\n",
       " array(-173673.62, dtype=float32),\n",
       " array(-174781.6, dtype=float32),\n",
       " array(-175906.42, dtype=float32),\n",
       " array(-177026.53, dtype=float32),\n",
       " array(-178155.61, dtype=float32),\n",
       " array(-179299.3, dtype=float32),\n",
       " array(-180445.77, dtype=float32),\n",
       " array(-181601.3, dtype=float32)]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan, nan]], requires_grad=True)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fld.W_fld"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "205d50f58f82a2b4d9ed38d5fc136ab30afd6ca8c2e73e92b6068ffbf36380d0"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
